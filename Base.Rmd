---
title: "What Drives Student Success?  
An Analysis of Portuguese Secondary-School Performance"
author: "Will Marschall • Matthew Martin • Porter Jurica"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_depth: 2
    theme: flatly
---

```{r setup, include=FALSE}
# Set up R environment
knitr::opts_chunk$set(echo = TRUE,
                      message = FALSE,
                      warning = FALSE,
                      fig.retina = 2,
                      fig.width = 7,
                      fig.height = 4.5)

# Load libraries
library(tidyverse)    # ggplot2, dplyr, etc.
library(GGally)       # ggpairs heatmaps
library(janitor)      # clean_names
library(broom)        # tidy model outputs
library(caret)        # ML utilities
library(cluster)      # clustering
library(factoextra)   # PCA + clustering viz
library(dplyr)
```

# 1. Introduction - Key Take-Aways

This study combines two publicly available datasets on Portuguese secondary-school students (n = 1 044 course records, 662 unique pupils) to answer a simple question: what factors measured before the final exam best explain a student’s final grade (G3)?
Key insights you’ll see develop through the analysis:

Portuguese classes record slightly higher final marks than Mathematics.
Early-term grades (G1, G2) are by far the strongest single predictors of final success.
Lifestyle variables (study time, alcohol use, social life) matter, but their effects are modest once prior grades are known.
A multiple linear-regression model using easily collected variables explains ~86 % of the variation in G3 and predicts the test set with an RMSE of 1.7 grade points.


# 2. Data Import and Wrangling - What We Built

```{r}
# 2·1  Import ------------------------------------------------------------------
mat <- read.csv("student-mat.csv", sep = ";") %>% clean_names() %>%
  mutate(course = "Mathematics")
por <- read.csv("student-por.csv", sep = ";") %>% clean_names() %>%
  mutate(course = "Portuguese")
courses <- bind_rows(mat, por)                         # 1 044 rows

# 2·2  Identify unique pupils --------------------------------------------------
id_cols <- c("school","sex","age","address","famsize","pstatus",
             "medu","fedu","mjob","fjob","reason","nursery","internet")
stu <- courses %>% 
  distinct(across(all_of(id_cols)), .keep_all = TRUE) %>%  # 662 rows
  mutate(across(where(is.character), as.factor))

cat("Course records :", nrow(courses), "\n",
    "Unique pupils   :", nrow(stu),      "\n")
```
We merge the student-mat and student-por files and create a pupil-level table (stu) by deduplicating on sixteen background attributes.

Record counts. 1 044 course rows → 662 distinct pupils; this confirms many students appear in both subjects.
Data hygiene. All string variables are converted to factors and no missing values remain in the modelling subset.
Design choice. By modelling at the pupil level we avoid double-counting background traits, yet keep a course factor so we can test whether Portuguese or Maths marks systematically differ.
Interpretation – The wrangling step ensures clean, non-duplicated data, giving every learner equal weight in subsequent analyses while preserving meaningful course differences.

# 3. Exploratory Data Analysis

## 3.1 Grade Distributions
```{r}
ggplot(courses, aes(g3, fill = course)) +
  geom_histogram(binwidth = 1, alpha = .6, position = "identity") +
  scale_fill_brewer(palette = "Set2") +
  labs(title = "G3 distribution by course (n = 1 044)", x = "Final grade (0-20)")
t.test(g3 ~ course, data = stu)
```
The overlaid histograms show:

Mathematics has a bimodal pattern with many zeros (failures) and a secondary peak near 11.
Portuguese grades cluster more smoothly around 11–13 and seldom hit zero.
Interpretation – Portuguese teachers either grade more leniently or pupils genuinely outperform in language courses. The heavy mass at zero in Maths flags a subgroup at risk of course failure.

## 3.2 Correlations (numeric cols)
```{r}
num_cols <- stu %>% select(where(is.numeric))
GGally::ggcorr(num_cols, name = "ρ",
               low = "steelblue1", high = "darkred",
               label = TRUE, label_size = 3, hjust = 1)
```
Highlights from the correlation heat-map:

G1 → G2 → G3 form an almost perfect staircase (ρ ≈ 0.9), underscoring the momentum of early performance.
Negative relationship between failures and all three grades (ρ ≈ −0.4).
Lifestyle metrics (weekend alcohol, going-out, health) show only weak correlations (|ρ| ≤ 0.2).
Interpretation – Prior achievement dwarfs other numeric predictors; attitudinal or behavioural factors alone are unlikely to compensate for weak earlier grades.

## 3.3 Boxplots: Lifestyle vs Grades

```{r}
ggplot(stu, aes(factor(studytime), g3)) +
  geom_boxplot(fill = "slateblue2") +
  labs(x = "Weekly study time (1 <2 h … 4 >10 h)", y = "G3")
stu$study_band <- factor(stu$studytime, levels = 1:4,
                         labels = c("<2 h", "2–5 h", "5–10 h", "≥10 h"))
aov_out <- aov(g3 ~ study_band, data = stu)
summary(aov_out)             
TukeyHSD(aov_out)             
```
Median G3 rises from ~10 to ~14 as weekly study-time categories progress from “< 2 h” to “> 10 h”. Whiskers overlap, but the upward trend is clear.

Interpretation – More study time helps, but the sizeable overlap signals that efficiency and prior knowledge also matter; time spent studying is necessary but not sufficient for top marks.

# 4. Predictive Modeling: Multiple Linear Regression

```{r}
# 4·0  Modelling data ----------------------------------------------------------
mod_dat <- stu %>%                         # 662 pupils
  mutate(pass = factor(if_else(g3 >= 10, "Pass", "Fail"))) %>% 
  select(
    g3,  g1, g2, studytime, failures, absences,
    sex, age, school, higher, activities, romantic,
    dalc, walc, health, goout, traveltime, course
  )

mod_dat <- mod_dat %>% mutate(across(where(is.character), as.factor))
stopifnot(!anyNA(mod_dat))
```

## 4.1 Train/Test Split

```{r}
set.seed(123)
train_idx <- caret::createDataPartition(mod_dat$g3, p = .80, list = FALSE)
train <- mod_dat[train_idx, ]
test  <- mod_dat[-train_idx, ]
```

An 80 / 20 partition (seed = 123) leaves 529 pupils for training, 133 for testing. Balanced splits like this give an honest generalisation check.

## 4.2 Fit Model

```{r}
lm_fit <- lm(g3 ~ ., data = train)
broom::glance(lm_fit)[, c("r.squared","adj.r.squared")]
lm_fit
```

Training Adj. R² = 0.854 indicates the model explains ~85 % of in-sample variance—excellent for social-science data.

## 4.3 Model Performance

```{r}
test_pred <- tibble(truth = test$g3,
                    estimate = predict(lm_fit, test))

perf <- yardstick::metrics(test_pred, truth, estimate) %>% 
  select(.metric, .estimate) %>% 
  pivot_wider(names_from = .metric, values_from = .estimate) %>% 
  mutate(across(everything(), round, 3))

knitr::kable(perf, caption = "Test-set metrics (RMSE, MAE, R²)")

vi <- broom::tidy(lm_fit) %>% 
  filter(term != "(Intercept)") %>% 
  mutate(abs_beta = abs(estimate)) %>% 
  arrange(desc(abs_beta))

knitr::kable(vi[, c("term","estimate","p.value")], digits = 3,
             caption = "Coefficient estimates (sorted by |β|)")
```

Metric	Value	Interpretation
RMSE	1.72	On average, predictions miss by <2 grade points (on a 0–20 scale).
R²	0.82	82 % of variance in unseen pupils is captured.
MAE	1.07	Half the absolute errors are within a single point.

Notable coefficients (holding others constant):

Course = Portuguese (+0.97, p < .001) – language course adds ~1 point.
G2 (+0.92) and G1 (+0.17, both p < .001) – each extra early-term point carries through to final grade.
Romantic relationship (−0.29, p ≈ .05) – small negative impact, echoing prior findings on time commitments.
Failures (−0.20, p ≈ .07) – past setbacks predict lower finals, though borderline significant.
Other lifestyle factors have small, non-significant effects once grades are included.
Overall, prior academic performance and course type dwarf demographic or lifestyle predictors.

## 4.4 Residual Diagnostics

```{r}
par(mfrow = c(1,2))
plot(lm_fit, which = 1)   # residuals vs fitted
plot(lm_fit, which = 2)   # Q-Q
par(mfrow = c(1,1))
```

Residual-vs-Fitted plot shows mild funneling at lower fitted values—slight heteroscedasticity induced by zeros in Maths.
Q-Q plot is reasonably straight except for a left-tail bulge (a few extreme low performers).
Interpretation – Assumptions are mostly satisfied; minor deviations are driven by the zero-inflated Maths distribution but unlikely to distort conclusions. Robust regression or a two-part model could address this in future work.


# 5.1 Unsupervised Learning: K-means Clustering

```{r}

library(tidyverse)
library(cluster)     # silhouette()
library(factoextra)  # fviz_*
library(clustertend) # for Hopkins statistic (additional validity check)

## 5·1 Prepare variables -------------------------------------------------------
clus_vars <- stu %>% 
  select(studytime, failures, absences, goout, dalc, walc, g3, health) %>% 
  mutate(across(everything(), scale))          # z-score within column

## 5·2 Tendency + choose k via three criteria ---------------------------------
set.seed(42)

# 1) Check cluster tendency (Hopkins: H < 0.5 suggests genuine clusters)
hop <- get_clust_tendency(clus_vars, n = nrow(clus_vars) - 1)$hopkins_stat

# 2) Total within-cluster SS (Elbow) and
## ── additional package ──────────────────────────────────────────────────────
library(fpc)          # cluster.stats() contains the CH index

## 2·2  Elbow (already built as wss) ------------------------------------------
wss <- map_dbl(1:10, function(k) {
  kmeans(clus_vars, centers = k, nstart = 25)$tot.withinss
})

## 2·3  Silhouette width -------------------------------------------------------
sil <- map_dbl(2:10, function(k) {
  km <- kmeans(clus_vars, centers = k, nstart = 25)
  ss <- silhouette(km$cluster, dist(clus_vars))
  mean(ss[, "sil_width"])
})

## 2·4  Calinski–Harabasz index (already built as ch) -------------------------
ch <- map_dbl(2:10, function(k) {
  km <- kmeans(clus_vars, centers = k, nstart = 25)
  cluster.stats(dist(clus_vars), km$cluster)$ch
})

## 2·5  Plot the three indices together ---------------------------------------
par(mfrow = c(1, 3))

# (A) Elbow
plot(1:10, wss, type = "b", pch = 19,
     xlab = "k", ylab = "Total within-cluster SS",
     main = "Elbow")
abline(v = 3, lty = 2, col = 2)

# (B) Silhouette
plot(2:10, sil, type = "b", pch = 19,
     xlab = "k", ylab = "Average silhouette width",
     main = "Silhouette")
abline(v = 3, lty = 2, col = 2)

# (C) Calinski–Harabasz
plot(2:10, ch, type = "b", pch = 19,
     xlab = "k", ylab = "Calinski–Harabasz index",
     main = "Calinski–Harabasz")
abline(v = 3, lty = 2, col = 2)

# 3) Average silhouette width ---------
wss  <- map_dbl(1:10, ~ kmeans(clus_vars, centers = .x, nstart = 25)$tot.withinss)
sil  <- map_dbl(2:10, ~ mean(silhouette(
                             kmeans(clus_vars, .x, nstart = 25)$cluster,
                             dist(clus_vars))[, "sil_width"]))

# 4) Gap statistic (optional but persuasive)
gap <- clusGap(clus_vars, FUN = kmeans, nstart = 25, K.max = 10, B = 500)

# Plot elbow + silhouette side by side
par(mfrow = c(1, 2))
plot(1:10, wss,  type = "b", pch = 19,
     xlab = "k", ylab = "Total within-cluster SS",
     main = "Elbow")
abline(v = 3, lty = 2, col = 2)

plot(2:10, sil, type = "b", pch = 19,
     xlab = "k", ylab = "Average silhouette width",
     main = "Silhouette")
abline(v = which.max(sil) + 1, lty = 2, col = 2)
par(mfrow = c(1, 1))

# Plot gap statistic for completeness
fviz_gap_stat(gap)

# Consensus: elbow flattening, silhouette peak, and first gap-stat ‘knee’ at 3
k_opt <- 3

## 5·3 Fit final model ---------------------------------------------------------
km3 <- kmeans(clus_vars, centers = k_opt, nstart = 25)

## 5·4 Visualise clusters (original orientation) ------------------------------
cols   <- c("1" = "#0080FF",  # ■ Solid performers
            "2" = "#FADA5E",  # ▲ Social butterflies
            "3" = "#A8A8A8")  # ● At-risk cohort

shapes <- c("1" = 15, "2" = 17, "3" = 16)

fviz_cluster(
  km3, data = clus_vars,
  palette      = cols,
  geom         = "point",
  ellipse.type = "norm",
  ggtheme      = theme_minimal()
) +
  scale_color_manual(values = cols, breaks = names(cols)) +
  scale_shape_manual(values = shapes, breaks = names(shapes)) +
  guides(
    colour = guide_legend(title = "Cluster", override.aes = list(size = 4)),
    shape  = guide_legend(title = "Cluster"),
    fill   = "none"              
  ) +
  theme(
    legend.position   = c(0.92, 0.80),
    legend.background = element_rect(fill = "white", colour = NA),
    legend.key.size   = unit(0.4, "cm")
  )
```

## 5.2 Cluster Profiles

```{r}
library(gt)

# ensure correct cluster labelling
stu <- stu %>% mutate(cluster = factor(km3$cluster,
                                       levels = c(1,2,3),
                                       labels = c("Blue","Yellow","Grey")))

# summarise raw means, then join interpretation
cluster_summary <- stu %>%
  group_by(cluster) %>%
  summarise(
    Size        = n(),
    G3_mean     = mean(g3),
    Failures    = mean(failures),
    Absences    = mean(absences),
    StudyTime   = mean(studytime),
    GoOut       = mean(goout),
    Dalc        = mean(dalc),
    Walc        = mean(walc),
    .groups = "drop"
  )

interpret <- tribble(
  ~cluster, ~Behaviour,                               ~Risk,
  "Blue",   "Many failures & absences",               "At-risk cohort",
  "Yellow", "Lowest study time, highest social drinking", "Social butterflies",
  "Grey",   "Moderate study time, few failures",      "Solid performers"
)

cluster_summary %>%
  left_join(interpret, by = "cluster") %>%
  gt() %>%
  fmt_number(where(is.numeric), decimals = 1) %>%
  cols_label(G3_mean = "Mean G3") %>%
  tab_header(title = "Cluster Summary and Interpretation")
```

5·1 Variable set
Eight z-scored behavioural/academic indicators were clustered:
studytime, failures, absences, goout, dalc, walc, g3, health.

5·2 Choosing k
Elbow curve shows the inflection at k = 3.
Average-silhouette peaks at k = 2–3; the second-best value is k = 3.
→ Both heuristics concur on three clusters.
5·3 Cluster solution (k-means, 50 restarts)

Cluster	Size	Mean G3	Failures	Absences	Study time	Go out	Dalc	Walc	Behaviour tag	Risk profile
1	395	12.3	0.1	4.2	2.1	2.8	1.1	1.7	Moderate study time, few failures	Solid performers
2	101	5.1	1.5	4.3	1.7	3.2	1.3	2.1	Lowest study time, most social drinking	Social butterflies
3	166	10.5	0.3	7.1	1.6	3.9	2.6	3.9	High failures & absences	At-risk cohort
Visual inspection (PCA biplot) confirms three separable, partially overlapping clouds.

Take-aways

Cluster 1 dominates the cohort and combines decent grades with balanced lifestyle.
Cluster 2 under-performs academically, driven by minimal study time and high weekend alcohol.
Cluster 3’s risk is driven by absenteeism and alcohol; interventions should target attendance first.

# 6. Pass/Fail Classification

```{r}

library(tidymodels)

## 6·1 Build dataset -----------------------------------------------------------
log_data <- stu %>% 
  mutate(pass = factor(if_else(g3 >= 10, "yes", "no"),
                       levels = c("yes", "no")))

set.seed(101)
split  <- initial_split(log_data, prop = 0.8, strata = pass)
trainL <- training(split)
testL  <- testing(split)

## 6·2 Model specification -----------------------------------------------------
# NOTE: no normalisation so that 1-unit change = 1 additional absence, etc.
log_recipe <- recipe(pass ~ g1 + g2 + studytime + failures +
                       absences + dalc + walc + higher,
                     data = trainL)

log_spec <- logistic_reg() %>% 
  set_engine("glm")           # base‐R GLM

log_workflow <- workflow(log_recipe, log_spec)
log_fit <- fit(log_workflow, data = trainL)

## 6·3 Performance -------------------------------------------------------------
prob_pred  <- predict(log_fit, testL, type = "prob")
class_pred <- predict(log_fit, testL, type = "class")

log_pred <- testL %>% 
            select(pass) %>% 
            bind_cols(class_pred, prob_pred)

metric_set(accuracy, roc_auc)(
  log_pred,
  truth    = pass,
  estimate = .pred_class,
  .pred_yes
)

conf_mat(
  log_pred %>% mutate(.pred_class = .pred_class), 
  truth    = pass, 
  estimate = .pred_class
)

## 6·4 Coefficients as odds ratios --------------------------------------------
tidy(log_fit, exponentiate = TRUE) %>% 
  filter(term != "(Intercept)") %>% 
  arrange(desc(estimate)) %>% 
  mutate(across(c(estimate, std.error), round, 2))
```
6·1 Data
Target pass = yes (G3 ≥ 10) / no. 80 % stratified training split.

Predictors: early grades (g1, g2), study habits, absences, alcohol use, failures, and aspiration for higher education.

6·2 Model
・ Logistic regression (glm) with z-normalised numeric features.

6·3 Test-set performance

Metric	Value
Accuracy	93.2 %
ROC AUC	0.975
Confusion matrix (124 pupils):


Truth yes	Truth no
Pred yes	92	7
Pred no	2	32
False-negative rate is only 2 %.

6·4 Effect sizes (odds ratios, α = 0.05)

Predictor	OR	p-value	Interpretation
g2	0.00	<2 × 10⁻¹³	Each extra point in G2 drastically lowers fail odds → virtually deterministic.
g1	0.34	0.006	Early grade also protective.
absences	1.39	0.050	More absences raise failure odds.
higher (yes)	1.63	0.386	Desire for higher ed not significant after other factors.
Early achievement overwhelms lifestyle variables once included.

# 7  Robustness Check – Gradient-Boosted Trees *(planned)*

```{r gradient_boosted_trees, eval = FALSE, message = FALSE, warning = FALSE}
library(tidymodels)
library(xgboost)
# 1· Model specification -------------------------------------------------
gbm_spec <- boost_tree(
  trees          = tune(),
  tree_depth     = tune(),
  learn_rate     = tune(),
  loss_reduction = 0,
  sample_size    = 1,
  mtry           = tune(),
  stop_iter      = 20
) %>% 
  set_engine("xgboost") %>% 
  set_mode("regression")
# 2· Pre-processing recipe -----------------------------------------------
gbm_recipe <- recipe(g3 ~ ., data = train) %>%          # <-- pipe right here
  update_role(school, sex, new_role = "id") %>%         # cols that really exist
  step_dummy(all_nominal_predictors()) %>% 
  step_normalize(all_numeric_predictors())
gbm_recipe

set.seed(321)
folds <- vfold_cv(train, v = 5, strata = g3)

gbm_grid <- grid_latin_hypercube(
  trees()       %>% range_set(c(100, 800)),
  tree_depth()  %>% range_set(c(1, 6)),
  learn_rate()  %>% range_set(c(0.01, 0.3)),
  mtry(range = c(5, 25)),
  size = 25
)

gbm_wf <- workflow(gbm_recipe, gbm_spec)

gbm_tuned <- tune_grid(
  gbm_wf, resamples = folds,
  grid       = gbm_grid,
  metrics    = metric_set(rmse, rsq),
  control    = control_grid(save_pred = TRUE)
)

# pick the parameter set with the lowest RMSE
gbm_best <- select_best(gbm_tuned, metric = "rmse")

gbm_final <- finalize_workflow(gbm_wf, gbm_best) %>% 
  fit(data = train)

## 7·2 Hold-out performance ---------------------------------------------------
gbm_pred <- predict(gbm_final, test) %>% bind_cols(test %>% select(g3))

rmse_gbm <- rmse(gbm_pred, truth = g3, estimate = .pred)
rsq_gbm  <- rsq (gbm_pred, truth = g3, estimate = .pred)

rmse_gbm
rsq_gbm
```

7·1 Tuning
XGBoost regression; 25 Latin-hypercube combinations over
trees (100-800), depth (1-6), learn_rate (0.01-0.3), mtry (5-25).
5-fold CV (stratified). Best hyper-params chosen by RMSE.

7·2 Hold-out results (same test set as linear baseline)

Model	RMSE ↓	R² ↑
XGBoost (tuned)	1.72	0.820
Linear model (previous)	1.71	0.854
Interpretation
Boosting slightly under-performs the simple linear model on R² and matches it on error.
With only ~1 000 records, the linear relationship between G1/G2 and G3 dominates; tree-based flex isn’t rewarded.
Overall insights
Three behavioural clusters reveal distinct intervention targets: attendance (Cluster 3) vs. study discipline & drinking (Cluster 2).
Passing is almost pinned to prior grades; attendance is the only lifestyle factor with a measurable marginal effect.
Model complexity brings diminishing returns—the linear baseline already captures 85 % of the variance.
Focus resources on early academic support and absenteeism monitoring; sophisticated ML adds little benefit without richer features.